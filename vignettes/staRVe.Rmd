---
title: "Spatio-temporal analysis of research vessel data"
author: "Ethan Lawler"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    fig_width: 6
    fig_height: 6
  rmarkdown::pdf_vignette:
    toc: true
    toc_depth: 3
    fig_width: 6
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{Spatio-temporal analysis of research vessel data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(staRVe)
```


# Fitting a model

We'll use a survey of bird counts, included in the package as the `bird_survey` data (which is an `sf` object containing location data).

```
data(bird_survey)
bird_survey
```

We can specify a model using the formula interface (see `?lm` or `?glm`), although formulae work a bit differently in the `staRVe` package.
A typical formula for a `staRVe` model will look like
`Count ~ mean(Speed+Distance) + time(Year,type="ar1")`.
Covariates, here `Speed` and `Distance`, affecting the mean of the response variable (in this case `TotalCount`) can be given through the `mean(...)` function. The inside of `mean(...)` also follows the formula interface.

The time effect is also specified through the formula, using the `time(Year,type="ar1")` function.
You can only specify one variable to be your time variable, and the three `type`s available are `independent`, `ar1`, and `rw`.

The spatial field nodes can be provided through a second `sf` object -- otherwise the default behaviour uses all the locations in the data as the nodes (WARNING: if there are a large number of unique locations, the model preparation can take a long time).
For our example, we'll use all of the locations in the data only from the year 2000 as our spatial field nodes.
```{r}
field_nodes<- subset(bird_survey,year == 2000)
```

Finally, you can specify number of parent nodes for each observation using the `n_neighbours` argument (shortened to just `n` below). We will also use `time(Year,type="ar1")` to set up independent spatial fields each year.

As we have count data, we will use a Poisson distribution with a log link.

```{r}
staRVe_in<- prepare_staRVe_input(cnt ~ time(year,type="ar1"),
  bird_survey,
  field_nodes,
  distribution = "poisson",
  link = "log",
  n=5)
```

Once `prepare_staRVe_input` has transformed the data into a suitable format, all that is left to do is run the `fit_staRVe` function.
This function takes care of all the routine bits of fitting a model with TMB: making the objective object (with `TMB::MakeADFun`), optimizing the model (with `nlminb`), and finding the standard errors of parameter estimates (with `TMB::sdreport`).
The function then returns the output of each of those three steps as elements of a list, as well as two `sf` objects, containing the estimated random field (with standard errors), and containing the data, estimated response (with standard errors), and residuals.

```{r}
(bird_fit<- fit_staRVe(staRVe_in,silent=T))
```

You should always check for model convergence, which you can do by looking at either `convergence(bird_fit)` (less information) or `opt(bird_fit)` (more information):

```{r}
opt(bird_fit)
```

A convergence code of 0, or a message saying something about `relative convergence` is a good sign.
If the message says `false converge` or `failed to converge`, then you should not trust the output of the model.
Of course, you should always look at the parameter and random effect estimates to make sure they make sense!
You can inspect the output of the model:

```{r}
obs_fit<- observation(bird_fit)
proc_fit<- process(bird_fit)
par_fit<- parameters(bird_fit)

obs_fit
plot(obs_fit)
```

The plots above are aggregated by time, but you can look at a plot for a single time unit:

```{r}
plot(subset(obs_fit,year == 2002))
```


Since they can take a significant time to compute, residuals are not included in the output of the `fit_staRVe` function.
They can be computed with the `residuals` function.

```{r,eval=F}
### Not evaluated
residuals(bird_fit)
```





## Making predictions

To get a useful picture of the estimated spatial field (which is usually the "result" of interest), we'll have to make predictions over a raster grid.
We'll use the `raster` package here, but the `fasterize` package also works just as well.
The `predict_staRVe` function takes a fitted model (a `staRVe` object) and a raster and produces predictions over the raster for each year present in the model.
If desired you can also specify specific years for prediction.
Predictions are made for the spatial random field, the response at the link scale, and the response on the response scale.
Standard errors for prediction are also given for all three.

First, we'll define a (quite coarse) raster object from the geographical extent of our data:
```{r}
library(raster)
raster_to_predict<- raster(bird_survey,nrow=10,ncol=10)
raster_to_predict[]<- 0
```
Then we can predict on that raster:
```{r}
raster_prediction<- predict_staRVe(bird_fit,raster_to_predict)
names(raster_prediction)
raster_prediction[[1]]
```

Since we the data we supplied to `prepare_staRVe_input` had multiple years, the resulting `raster_prediction` is a list with a raster object for each year.


# Some theory and some terminology

Now that we've seen how to fit a model, let's actually take a look at the model we're fitting and the pieces of data that live under the hood.

## A description of the model

The current implementation supports a univariate spatial field at a single point in time, with no covariate information.
It's limited, but easily extendable, and understanding the foundation model will go a long way in feeling comfortable with more complicated ones.

For now the model assumes we are looking at presence-absence data, as a presence-absence model can be more robust to data quality.
Changing the model to analyze either counts or biomass would be as simple as changing the observation distribution and link function.
All of the underlying mechanics would be exactly the same as the presence-absence model.
So be aware of the aforementioned limitations, and know that there is a sleugh of items left on the to-do list.

The foundation model (univariate, single year, no covariates) for presence-absence data can be described by the following equations:

\[
\begin{aligned}
    Y_{i}\left(\mathbf{s}\right) \vert w\left(\mathbf{s}\right)
        &\sim \text{Bernoulli}\left(
                    p\left(\mathbf{s}\right) =  \text{invlogit}\left[\mu + w\left(\mathbf{s}\right)\right]
                \right) \\
    W\left(\mathbf{S}\right)
        &\sim \text{NNGP}\left(0,~C\left(\mathbf{s}_{1},\mathbf{s}_{2}\right)\right)
\end{aligned}
\]

Let's piece this together step by step.
The first equation, called the observation equation, involves the $i$th data point at location $\mathbf{s}$, denoted $Y_{i}\left(\mathbf{s}\right)$, conditional on a random field value at the same location, given by $w\left(\mathbf{s}\right)$.
Since our data here is presence-absence data, our response variable $Y$ can be either a `0` for an absence or a `1` for presence.
That's why we use the Bernoulli distribution.
This distribution gives the probability of a presence, where that probability is given, in this model, by the value
$\text{invlogit}\left[\mu + w\left(\mathbf{s}\right)\right].$
The invlogit function is called the link function, and takes values from $(-\infty,\infty)$ and transforms them to probabilities in the interval $(0,1)$.
Finally the parameter $\mu$ controls the overall (non-spatial) probability of a presence, while the random field $w\left(\mathbf{s}\right)$ controls how this probability changes with location.

The invlogit link function frees up the values of $\mu$ and $w\left(\mathbf{s}\right)$ to take on any value in the real numbers.
This freedom sets up the spatial random field $W\left(\mathbf{s}\right)$ to follow a Gaussian process.
The second equation above, called the process equation, describes this spatial random field.
(You might notice I use both upper-case $W$ and lower-case $w$ -- the big $W$ stands for a *random variable* which is described by a probability distribution; the small $w$ is an actual number that stands for a *realization* of the big $W$.)

A Gaussian process, shortened to GP, is written as
\[
    \text{GP}\left(\mu\left(\mathbf{s}\right),
        C\left(\mathbf{s}_{1},\mathbf{s}_{2}\right)
    \right).
\]
The parameter $\mu\left(\mathbf{s}\right)$ is actually a function which gives the mean of the spatial random field at each location.
In practice the mean fucntion $\mu\left(\mathbf{s}\right)$ is commonly fixed equal to 0.
Otherwise the parameter $\mu$ in the observation equation can be confused with the mean function for the Gaussian process.

The second parameter, $C\left(\mathbf{s}_{1},\mathbf{s}_{2}\right)$, is also a function.
This second function describes the covariance or correlation of the spatial random field.
If you give the covariance function two locations (they can even be the same location if you want), it will tell you if you should expect the random field at those two points to be similar.
High values of the covariance function lead to the random field at those locations to be similar.

While a Gaussian process is actually a function over the entire spatial region of interest, it is common practice to estimate it only at a certain number of points.
These points are sometimes called knots or nodes.
In a slight abuse of terminology, when I refer to a Gaussian process that could mean either the full function over the entire spatial region or just the knots.
However, when talking about the locations of a Gaussian process that refers specifically to the locations of the knots.

### Nearest neighbour Gaussian processes

Now, you might be wondering why the process equation above is written as
\[
W\left(\mathbf{S}\right)
    \sim \text{NNGP}\left(0,~C\left(\mathbf{s}_{1},\mathbf{s}_{2}\right)\right)
\]
with an NNGP instead of a GP.
To make the model actually work in a reasonable amount of time, our model uses a *nearest neighbour Gaussian process* instead of a vanilla *Gaussian process*.
The nearest neighbour Gaussian process, shortened to NNGP, needs much less computing power that the vanilla Gaussian process, but is a bit harder to fully understand.

To understand how a NNGP works compared a vanilla GP, consider two large parties with 100 people (or some other large number) in attendance each.
The hosts of the parties, Marc and Rachel, don't want anybody to feel left out, so they decide that they are going to introduce their guests to other guests.

Marc lives on Gaussian boulevard, and decides he wants to introduce every guest to every other guest.
That way, Marc argues, everybody will get to know each other.
After the 50th guest arrives, Marc has spent all of his time introducing people.
By the end of the night, Marc is exhausted and hasn't had any time to enjoy the party himself!
His guests, however, had a great time getting to know so many people.

Rachel, on the other hand, lives on nearest neighbour Gaussian avenue (these city planners should really think of shorter street names!).
Instead of introducing all of the guests to each other, Rachel decides she is only going to introduce people who live close to each other.
People who live close together, Rachel reasons, will probably benefit more from knowing each other, but people who live far apart might not get out of the relationship.
So Rachel only introduces each guest to the 5 people who live nearest to them.
At the end of Rachel's party, all the guests are *almost* as happy as they would have been at Marc's.
But(!) Rachel only spent a small amount of time introducing people and was able to enjoy her own party with her guests.

Translating this back from Marc and Rachel's parties, the 100 guests are 100 locations of special interest.
When trying to find the value of the random field at a specific location $\mathbf{s}$, Marc's Gaussian process method uses the information of all the 100 locations of interest to predict at $\mathbf{s}$.
Rachel's nearest neighbour Gaussian process method, on the other hand, uses only the 10 closest locations to $\mathbf{s}$ to predict at $\mathbf{s}$
Just as with the happiness of the guests, Rachel's nearest neighbour method works *almost* as well as Marc's vanilla Gaussian method.

\

There is one more technical detail in the nearest neighbour Gaussian process: information is only allowed to flow in one direction (e.g. from left to right).
To accomplish this, we create a so called directed acyclic graph (or DAG for short) which described both the flow of information and the factorization of the joint likelihood into conditional likelihoods.

In our model, each point represents a location for which we're finding the value of the spatial random field (the $w\left(\mathbf{s}\right)$).
We compute the covariance function once for each arrow in the graph.
We can also predict at a new location by finding the points on the graph closest to it and drawing arrows from those points to the new one.
Once we've predicted the value $w\left(\mathbf{s}\right)$ of the spatial random field at the location $\mathbf{s}$, we can plug it into the link function of the observation equation
\[
    p\left(\mathbf{s}\right) = \text{invlogit}\left[\mu+w\left(\mathbf{s}\right)\right]
\]
to predict the probability of a presence $p\left(\mathbf{s}\right)$ at the location $\mathbf{s}$.


## What actually happens when you fit a model?

The only data that we have to plug in to the model are the presence-absence observations $Y_{i}\left(\mathbf{s}\right)$.
Each observation is recorded as either a `0` for absence or a `1` for presence.

We have no direct data for the random spatial field $W\left(\mathbf{s}\right)$, but since the spatial field is connected to the observations through the link function we can still perform inference on the field.
The best way to think about the random spatial field is that, through the process equation, the random field gives the necessary spatial structure to the data.

Behind the scenes, we apply a technical trick in fitting the model.
By 'integrating out' the spatial random field, which in a more general form we call 'marginalizing over the random effects', we can introduce spatial structure to the observations without having to worry about the actual value of the spatial random field.
After finding the parameters using this marginal model, we then go back and estimate the value of random field.
